{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in=10\n",
    "n_out=10\n",
    "n_hidden=10\n",
    "n_samples=300\n",
    "learning_rate=300\n",
    "momentum=0.9\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(x):\n",
    "    return -np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,t,V,W,bv,bw):\n",
    "    #forward\n",
    "    A=np.dot(x,V)+bv\n",
    "    Z=np.tanh(A)\n",
    "    B=np.dot(Z,W)+bw\n",
    "    V=sigmoid(B)\n",
    "    #backward\n",
    "    EW=V-t\n",
    "    EV=tanh_prime(A)*np.dot(W,EW)\n",
    "    dW=np.outer(Z,EW)\n",
    "    dV=np.outer(x,EV)\n",
    "    loss=-np.mean(t*np.log(V)+(1-t)*np.log(1-V))\n",
    "    #use error as gradient for each layer\n",
    "    return loss,(dV,dW,EV,EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,V,W,bv,bw):\n",
    "    A=np.dot(x,V)+bv\n",
    "    B=np.dot(np.tanh(A),W)+bw\n",
    "    return (sigmoid(B)>0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create initial parameters\n",
    "#initialization is crucial for first order methods\n",
    "V=np.random.normal(scale=0.1,size=(n_in,n_hidden))\n",
    "W=np.random.normal(scale=0.1,size=(n_hidden,n_out))\n",
    "bv=np.zeros(n_hidden)\n",
    "bw=np.zeros(n_out)\n",
    "params=[V,W,bv,bw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some data\n",
    "x=np.random.binomial(1,0.5,(n_samples,n_in))\n",
    "t=x^1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7ba15436d5d1>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1+np.exp(-x))\n",
      "<ipython-input-33-93deec8be6fd>:12: RuntimeWarning: divide by zero encountered in log\n",
      "  loss=-np.mean(t*np.log(V)+(1-t)*np.log(1-V))\n",
      "<ipython-input-33-93deec8be6fd>:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss=-np.mean(t*np.log(V)+(1-t)*np.log(1-V))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Loss:     nan,Time:0.046875s\n",
      "Epoch:1,Loss:     nan,Time:0.015625s\n",
      "Epoch:2,Loss:     nan,Time:0.031250s\n",
      "Epoch:3,Loss:     nan,Time:0.015625s\n",
      "Epoch:4,Loss:     nan,Time:0.015625s\n",
      "Epoch:5,Loss:     nan,Time:0.031250s\n",
      "Epoch:6,Loss:     nan,Time:0.015625s\n",
      "Epoch:7,Loss:     nan,Time:0.015625s\n",
      "Epoch:8,Loss:     nan,Time:0.031250s\n",
      "Epoch:9,Loss:     nan,Time:0.015625s\n",
      "Epoch:10,Loss:     nan,Time:0.015625s\n",
      "Epoch:11,Loss:     nan,Time:0.015625s\n",
      "Epoch:12,Loss:     nan,Time:0.015625s\n",
      "Epoch:13,Loss:     nan,Time:0.031250s\n",
      "Epoch:14,Loss:     nan,Time:0.015625s\n",
      "Epoch:15,Loss:     nan,Time:0.015625s\n",
      "Epoch:16,Loss:     nan,Time:0.031250s\n",
      "Epoch:17,Loss:     nan,Time:0.015625s\n",
      "Epoch:18,Loss:     nan,Time:0.015625s\n",
      "Epoch:19,Loss:     nan,Time:0.031250s\n",
      "Epoch:20,Loss:     nan,Time:0.015625s\n",
      "Epoch:21,Loss:     nan,Time:0.015625s\n",
      "Epoch:22,Loss:     nan,Time:0.031250s\n",
      "Epoch:23,Loss:     nan,Time:0.015625s\n",
      "Epoch:24,Loss:     nan,Time:0.015625s\n",
      "Epoch:25,Loss:     nan,Time:0.031250s\n",
      "Epoch:26,Loss:     nan,Time:0.015625s\n",
      "Epoch:27,Loss:     nan,Time:0.015625s\n",
      "Epoch:28,Loss:     nan,Time:0.031250s\n",
      "Epoch:29,Loss:     nan,Time:0.015625s\n",
      "Epoch:30,Loss:     nan,Time:0.015625s\n",
      "Epoch:31,Loss:     nan,Time:0.031250s\n",
      "Epoch:32,Loss:     nan,Time:0.015625s\n",
      "Epoch:33,Loss:     nan,Time:0.015625s\n",
      "Epoch:34,Loss:     nan,Time:0.046875s\n",
      "Epoch:35,Loss:     nan,Time:0.015625s\n",
      "Epoch:36,Loss:     nan,Time:0.031250s\n",
      "Epoch:37,Loss:     nan,Time:0.015625s\n",
      "Epoch:38,Loss:     nan,Time:0.015625s\n",
      "Epoch:39,Loss:     nan,Time:0.031250s\n",
      "Epoch:40,Loss:     nan,Time:0.015625s\n",
      "Epoch:41,Loss:     nan,Time:0.015625s\n",
      "Epoch:42,Loss:     nan,Time:0.031250s\n",
      "Epoch:43,Loss:     nan,Time:0.015625s\n",
      "Epoch:44,Loss:     nan,Time:0.015625s\n",
      "Epoch:45,Loss:     nan,Time:0.031250s\n",
      "Epoch:46,Loss:     nan,Time:0.015625s\n",
      "Epoch:47,Loss:     nan,Time:0.015625s\n",
      "Epoch:48,Loss:     nan,Time:0.031250s\n",
      "Epoch:49,Loss:     nan,Time:0.015625s\n",
      "Epoch:50,Loss:     nan,Time:0.031250s\n",
      "Epoch:51,Loss:     nan,Time:0.015625s\n",
      "Epoch:52,Loss:     nan,Time:0.015625s\n",
      "Epoch:53,Loss:     nan,Time:0.046875s\n",
      "Epoch:54,Loss:     nan,Time:0.015625s\n",
      "Epoch:55,Loss:     nan,Time:0.015625s\n",
      "Epoch:56,Loss:     nan,Time:0.031250s\n",
      "Epoch:57,Loss:     nan,Time:0.015625s\n",
      "Epoch:58,Loss:     nan,Time:0.015625s\n",
      "Epoch:59,Loss:     nan,Time:0.031250s\n",
      "Epoch:60,Loss:     nan,Time:0.015625s\n",
      "Epoch:61,Loss:     nan,Time:0.031250s\n",
      "Epoch:62,Loss:     nan,Time:0.015625s\n",
      "Epoch:63,Loss:     nan,Time:0.031250s\n",
      "Epoch:64,Loss:     nan,Time:0.015625s\n",
      "Epoch:65,Loss:     nan,Time:0.015625s\n",
      "Epoch:66,Loss:     nan,Time:0.031250s\n",
      "Epoch:67,Loss:     nan,Time:0.015625s\n",
      "Epoch:68,Loss:     nan,Time:0.015625s\n",
      "Epoch:69,Loss:     nan,Time:0.031250s\n",
      "Epoch:70,Loss:     nan,Time:0.015625s\n",
      "Epoch:71,Loss:     nan,Time:0.031250s\n",
      "Epoch:72,Loss:     nan,Time:0.015625s\n",
      "Epoch:73,Loss:     nan,Time:0.031250s\n",
      "Epoch:74,Loss:     nan,Time:0.031250s\n",
      "Epoch:75,Loss:     nan,Time:0.031250s\n",
      "Epoch:76,Loss:     nan,Time:0.015625s\n",
      "Epoch:77,Loss:     nan,Time:0.031250s\n",
      "Epoch:78,Loss:     nan,Time:0.031250s\n",
      "Epoch:79,Loss:     nan,Time:0.015625s\n",
      "Epoch:80,Loss:     nan,Time:0.031250s\n",
      "Epoch:81,Loss:     nan,Time:0.015625s\n",
      "Epoch:82,Loss:     nan,Time:0.015625s\n",
      "Epoch:83,Loss:     nan,Time:0.031250s\n",
      "Epoch:84,Loss:     nan,Time:0.031250s\n",
      "Epoch:85,Loss:     nan,Time:0.015625s\n",
      "Epoch:86,Loss:     nan,Time:0.031250s\n",
      "Epoch:87,Loss:     nan,Time:0.031250s\n",
      "Epoch:88,Loss:     nan,Time:0.015625s\n",
      "Epoch:89,Loss:     nan,Time:0.015625s\n",
      "Epoch:90,Loss:     nan,Time:0.031250s\n",
      "Epoch:91,Loss:     nan,Time:0.015625s\n",
      "Epoch:92,Loss:     nan,Time:0.031250s\n",
      "Epoch:93,Loss:     nan,Time:0.015625s\n",
      "Epoch:94,Loss:     nan,Time:0.015625s\n",
      "Epoch:95,Loss:     nan,Time:0.031250s\n",
      "Epoch:96,Loss:     nan,Time:0.015625s\n",
      "Epoch:97,Loss:     nan,Time:0.046875s\n",
      "Epoch:98,Loss:     nan,Time:0.015625s\n",
      "Epoch:99,Loss:     nan,Time:0.031250s\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "for epoch in range(100):\n",
    "    err=[]\n",
    "    upd=[0]*len(params)\n",
    "    t0=time.process_time()\n",
    "    for i in range (x.shape[0]):\n",
    "        loss,grad=train(x[i],t[i],*params)\n",
    "        for j in range (len(params)):\n",
    "            params[j]-=upd[j]\n",
    "        for j in range (len(params)):\n",
    "            upd[j]=learning_rate*grad[j]+momentum*upd[j]\n",
    "        err.append(loss) \n",
    "    print('Epoch:%d,Loss:%8f,Time:%4fs'\n",
    "         %(epoch,np.mean(err),time.process_time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 1 0 0 0]\n",
      "[1 0 0 0 1 0 0 1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7ba15436d5d1>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "x=np.random.binomial(1,0.5,n_in)\n",
    "print(x)\n",
    "print(predict(x,*params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
